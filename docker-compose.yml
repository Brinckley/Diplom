version: '2.1'
services:

  consul:
    container_name: consul
    image: consul:1.1.0
    ports:
      - 8500:8500
    networks: [ "go_develop" ] 

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    ports:
      - "5181:5181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 5181
      ZOOKEEPER_TICK_TIME: 2000
    networks: [ "go_develop" ]

  # reachable on 9092 from the host and on 29092 from inside docker compose
  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - '9092:9092'
    expose:
      - '29092'
    environment:
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:5181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: '1'
      KAFKA_MIN_INSYNC_REPLICAS: '1'
    networks: [ "go_develop" ]

  init-kafka:
    image: confluentinc/cp-kafka:latest
    restart: on-failure
    depends_on:
      - kafka
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      # blocks until kafka is reachable
      kafka-topics --bootstrap-server kafka:29092 --list
      echo -e 'Creating kafka topics'
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic Artist --replication-factor 1 --partitions 3
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic Album  --replication-factor 1 --partitions 3
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic Track  --replication-factor 1 --partitions 3
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic Event  --replication-factor 1 --partitions 3
      echo -e 'Successfully created the following topics:'
      kafka-topics --bootstrap-server kafka:29092 --list
      "
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics --bootstrap-server kafka:29092 --topic Artist --describe", "kafka-topics --bootstrap-server kafka:29092 --topic Album --describe", "kafka-topics --bootstrap-server kafka:29092 --topic Track --describe", "kafka-topics --bootstrap-server kafka:29092 --topic Event --describe"]
      interval: 2s
      timeout: 2s
      retries: 15
    networks: [ "go_develop" ]

  db:
    image: postgres:latest
    restart: always
    command: "-c logging_collector=on"
    ports:
      - "5436:5432"
    expose:
      - "5432"
    env_file:
      - ./env/database.env
    volumes:
      - ./data_postgres/database/postgres/data:/var/lib/postgresql/data
      - ./docker_postgres_init.sql:/docker-entrypoint-initdb.d/docker_postgres_init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 15
    networks: [ "go_develop" ]

  producer:
    build: 
      context: ./apps/producer
    depends_on:
      init-kafka:
        condition: service_healthy
    env_file:
      - ./env/producer.env
    networks: [ "go_develop" ]

  consumer:
    build: 
      context: ./apps/consumer
    depends_on:
      db:
        condition: service_healthy
      init-kafka:
        condition: service_healthy
    env_file:
      - ./env/consumer.env
    networks: [ "go_develop" ]

  tgclient:
    build: 
      context: ./apps/tgclient
    depends_on:
      db:
        condition: service_healthy
    env_file:
      - ./env/tgclient.env
    networks: [ "go_develop" ]

  elasticsearch:
    container_name: elasticsearch
    image: docker.elastic.co/elasticsearch/elasticsearch:8.6.2
    ports:
      - 9200:9200
    volumes:
      - ./data_elasticsearch/esdata1:/var/lib/elasticsearch/esdata1
    environment:
      - cluster.name=docker-cluster
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - ES_JAVA_OPTS=-Xms1g -Xmx1g
      - xpack.security.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    restart: always
    networks: [ "go_develop" ]

  fetcher:
    build: 
      context: ./apps/events-fetcher
    depends_on:
      db:
        condition: service_healthy
      elasticsearch:
        condition: service_started
    env_file:
      - ./env/fetcher.env
    networks: [ "go_develop" ]

  checker:
    build: 
      context: ./apps/checker
    depends_on:
      db:
        condition: service_healthy
      elasticsearch:
        condition: service_started
 #     init-kafka:
 #       condition: service_healthy
    env_file:
      - ./env/checker.env
    networks: [ "go_develop" ]

networks:
    go_develop:
        driver: bridge